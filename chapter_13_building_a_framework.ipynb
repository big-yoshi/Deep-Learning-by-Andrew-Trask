{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from chp13 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "target = np.array([[0],[1],[0],[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0_1 = np.random.rand(2,3)\n",
    "w_1_2 = np.random.rand(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.066439994622395\n",
      "0.4959907791902342\n",
      "0.4180671892167177\n",
      "0.35298133007809646\n",
      "0.2972549636567377\n",
      "0.2492326038163328\n",
      "0.20785392075862477\n",
      "0.17231260916265176\n",
      "0.14193744536652986\n",
      "0.11613979792168384\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    l1 = data.dot(w_0_1)\n",
    "    l2 = l1.dot(w_1_2)\n",
    "    diff = (l2-target)\n",
    "    sqdiff = (diff * diff)\n",
    "    loss = sqdiff.sum(0) #MSE loss\n",
    "    l1_grad = diff.dot(w_1_2.transpose()) # this is the backprop piece\n",
    "    w_1_2_update = l1.transpose().dot(diff)\n",
    "    w_0_1_update = data.transpose().dot(l1_grad)\n",
    "    \n",
    "    w_1_2 -= w_1_2_update * 0.1 # 0.1 is the dropout\n",
    "    w_0_1 -= w_0_1_update * 0.1\n",
    "    print(loss[0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# my version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = list()\n",
    "w.append(Tensor(np.random.rand(2,3), autograd=True))\n",
    "w.append(Tensor(np.random.rand(3,1), autograd=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.19432764]\n",
      "[0.69555247]\n",
      "[0.41681785]\n",
      "[0.26511189]\n",
      "[0.16699383]\n",
      "[0.10237465]\n",
      "[0.06048975]\n",
      "[0.03436548]\n",
      "[0.0188295]\n",
      "[0.00999548]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    pred = data.mm(w[0]).mm(w[1]) # predict\n",
    "    loss = ((pred - target) * (pred-target)).sum(0) # compare\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))# learn\n",
    "    \n",
    "    for w_ in w:\n",
    "        w_.data -= w_.grad.data * 0.1\n",
    "        w_.grad.data *= 0\n",
    "    print(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.60308555]\n",
      "[0.90713554]\n",
      "[0.23779431]\n",
      "[0.10544739]\n",
      "[0.06594947]\n",
      "[0.04025429]\n",
      "[0.0238281]\n",
      "[0.01372652]\n",
      "[0.00772696]\n",
      "[0.00426826]\n"
     ]
    }
   ],
   "source": [
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "w = list()\n",
    "w.append(Tensor(np.random.rand(2,3), autograd=True))\n",
    "w.append(Tensor(np.random.rand(3,1), autograd=True))\n",
    "\n",
    "optim = SGD(parameters=w,alpha=0.1)\n",
    "\n",
    "for i in range(10):\n",
    "    pred = data.mm(w[0]).mm(w[1])\n",
    "    loss = ((pred- target) * (pred-target)).sum(0)\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.33428272]\n",
      "[0.06743796]\n",
      "[0.0521849]\n",
      "[0.04079507]\n",
      "[0.03184365]\n",
      "[0.02479336]\n",
      "[0.01925443]\n",
      "[0.01491699]\n",
      "[0.01153118]\n",
      "[0.00889602]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy\n",
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "model = Sequential([Linear(2,3), Linear(3,1)])\n",
    "\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.05)\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    # Predict\n",
    "    pred = model.forward(data)\n",
    "    \n",
    "    # Compare\n",
    "    loss = ((pred - target)*(pred - target)).sum(0)\n",
    "    \n",
    "    # Learn\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.33428272]\n",
      "[0.06743796]\n",
      "[0.0521849]\n",
      "[0.04079507]\n",
      "[0.03184365]\n",
      "[0.02479336]\n",
      "[0.01925443]\n",
      "[0.01491699]\n",
      "[0.01153118]\n",
      "[0.00889602]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "model = Sequential([Linear(2,3), Linear(3,1)])\n",
    "criterion = MSELoss()\n",
    "optim = SGD(parameters=model.get_parameters(), alpha= 0.05)\n",
    "for i in range(10):\n",
    "    pred = model.forward(data)\n",
    "    loss = criterion.forward(pred, target)\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.06372865]\n",
      "[0.75148144]\n",
      "[0.57384259]\n",
      "[0.39574294]\n",
      "[0.2482279]\n",
      "[0.15515294]\n",
      "[0.10423398]\n",
      "[0.07571169]\n",
      "[0.05837623]\n",
      "[0.04700013]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "model = Sequential([Linear(2,3),Tanh(), Linear(3,1), Sigmoid()])\n",
    "criterion = MSELoss()\n",
    "optim = SGD(parameters=model.get_parameters(),alpha=1)\n",
    "\n",
    "for i in range(10):\n",
    "    pred = model.forward(data)\n",
    "    loss = criterion.forward(pred, target)\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03627183, -0.06432653, -0.0860282 , -0.09806237,  0.07840993],\n",
       "       [-0.07330696,  0.05583839,  0.08503269,  0.04303575, -0.00183628],\n",
       "       [-0.00610034,  0.07654186, -0.00231799, -0.0170865 , -0.06517761],\n",
       "       ...,\n",
       "       [-0.05424691,  0.03699274, -0.01992327, -0.07552934,  0.069757  ],\n",
       "       [-0.03305273,  0.0928904 ,  0.06222329,  0.06001063,  0.06538392],\n",
       "       [-0.01837391, -0.08355829,  0.09900976,  0.09283013, -0.00645979]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.random.rand(1000, 5) -0.5) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2.]\n",
      " [1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "x = Tensor(np.eye(5), autograd=True)\n",
    "x.index_select(Tensor([[1,2,3],[2,3,4]])).backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96438459]\n",
      "[0.30384689]\n",
      "[0.15730965]\n",
      "[0.10253432]\n",
      "[0.07482662]\n",
      "[0.05837488]\n",
      "[0.04758131]\n",
      "[0.04000142]\n",
      "[0.03440938]\n",
      "[0.03012687]\n"
     ]
    }
   ],
   "source": [
    "data = Tensor(np.array([1,2,1,2]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "embed = Embedding(5,3)\n",
    "model = Sequential([embed, Tanh(), Linear(3,1), Sigmoid()])\n",
    "criterion = MSELoss()\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.5)\n",
    "\n",
    "for i in range(10):\n",
    "    pred = model.forward(data)\n",
    "    loss = criterion.forward(pred, target)\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3885032434928422\n",
      "0.9558181509266036\n",
      "0.6823083585795604\n",
      "0.509525996749312\n",
      "0.39574491472895856\n",
      "0.31752527285348264\n",
      "0.2617222861964216\n",
      "0.22061283923954225\n",
      "0.18946427334830068\n",
      "0.16527389263866668\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "# data indices\n",
    "data = Tensor(np.array([1,2,1,2]), autograd=True)\n",
    "# target indices\n",
    "target = Tensor(np.array([0,1,0,1]), autograd=True)\n",
    "model = Sequential([Embedding(3,3),Tanh(), Linear(3,4)])\n",
    "criterion = CrossEntropyLoss()\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.1)\n",
    "\n",
    "for i in range(10):\n",
    "    pred = model.forward(data)\n",
    "    loss = criterion.forward(pred, target)\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now lets use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,random,math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "f = open('data/tasksv11/en/qa1_single-supporting-fact_train.txt','r')\n",
    "raw = f.readlines()\n",
    "f.close()\n",
    "\n",
    "tokens = list()\n",
    "for line in raw[0:1000]:\n",
    "    tokens.append(line.lower().replace(\"\\n\",\"\").split(\" \")[1:])\n",
    "\n",
    "new_tokens = list()\n",
    "for line in tokens:\n",
    "    new_tokens.append(['-'] * (6 - len(line)) + line)\n",
    "\n",
    "tokens = new_tokens\n",
    "\n",
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "\n",
    "vocab = list(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "    \n",
    "def words2indices(sentence):\n",
    "    idx = list()\n",
    "    for word in sentence:\n",
    "        idx.append(word2index[word])\n",
    "    return idx\n",
    "\n",
    "indices = list()\n",
    "for line in tokens:\n",
    "    idx = list()\n",
    "    for w in line:\n",
    "        idx.append(word2index[w])\n",
    "    indices.append(idx)\n",
    "\n",
    "data = np.array(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(vocab_size=len(vocab), dim=16)\n",
    "model = RNNCell(n_inputs=16,n_hidden=16, n_output=len(vocab))\n",
    "criterion = CrossEntropyLoss()\n",
    "params = model.get_parameters() + embed.get_parameters()\n",
    "optim = SGD(parameters=params, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4867304084830028 % Correct: 0.0\n",
      "Loss: 0.1782472256417358 % Correct: 0.21\n",
      "Loss: 0.16358863350759348 % Correct: 0.3\n",
      "Loss: 0.14174073412286864 % Correct: 0.36\n",
      "Loss: 0.13615299451500776 % Correct: 0.37\n"
     ]
    }
   ],
   "source": [
    "for iter in range(1000):\n",
    "    batch_size = 100\n",
    "    total_loss = 0\n",
    "    \n",
    "    hidden = model.init_hidden(batch_size=batch_size)\n",
    "\n",
    "    for t in range(5):\n",
    "        input = Tensor(data[0:batch_size,t], autograd=True)\n",
    "        rnn_input = embed.forward(input=input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "\n",
    "    target = Tensor(data[0:batch_size,t+1], autograd=True)    \n",
    "    loss = criterion.forward(output, target)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    total_loss += loss.data\n",
    "    if(iter % 200 == 0):\n",
    "        p_correct = (target.data == np.argmax(output.data,axis=1)).mean()\n",
    "        print(\"Loss:\",total_loss / (len(data)/batch_size),\"% Correct:\",p_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- \n",
      "- mary \n",
      "- mary moved \n",
      "- mary moved to \n",
      "- mary moved to the \n",
      "Context: - mary moved to the \n",
      "Pred: garden.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "hidden = model.init_hidden(batch_size=batch_size)\n",
    "for t in range(5):\n",
    "    input = Tensor(data[0:batch_size,t], autograd=True)\n",
    "    rnn_input = embed.forward(input=input)\n",
    "    output, hidden = model.forward(input=rnn_input,hidden=hidden)\n",
    "target = Tensor(data[0:batch_size,t+1],autograd=True)\n",
    "loss = criterion.forward(output, target)\n",
    "ctx = \"\"\n",
    "for idx in data[0:batch_size][0][0:-1]:\n",
    "    ctx += vocab[idx] + \" \"\n",
    "    print(ctx)\n",
    "print(\"Context:\",ctx)\n",
    "print(\"Pred:\",vocab[output.data.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
